{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as onp\n",
    "import jax.numpy as np\n",
    "from jax import random, grad, vmap, jit, jacfwd, jacrev\n",
    "from jax.example_libraries import optimizers\n",
    "\n",
    "from jax.nn import relu\n",
    "from jax import lax\n",
    "from jax.flatten_util import ravel_pytree\n",
    "import itertools\n",
    "from functools import partial\n",
    "from torch.utils import data\n",
    "from tqdm import trange\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the neural net\n",
    "def init_layer(key, d_in, d_out):\n",
    "    k1, k2 = random.split(key)\n",
    "    glorot_stddev = 1. / np.sqrt((d_in + d_out) / 2.)\n",
    "    W = glorot_stddev * random.normal(k1, (d_in, d_out))\n",
    "    b = np.zeros(d_out)\n",
    "    return W, b\n",
    "\n",
    "\n",
    "def MLP(layers, activation=relu):\n",
    "    ''' Vanilla MLP'''\n",
    "\n",
    "    def init(rng_key):\n",
    "        key, *keys = random.split(rng_key, len(layers))\n",
    "        params = list(map(init_layer, keys, layers[:-1], layers[1:]))\n",
    "        return params\n",
    "\n",
    "    def apply(params, inputs):\n",
    "        for W, b in params[:-1]:\n",
    "            outputs = np.dot(inputs, W) + b\n",
    "            inputs = activation(outputs)\n",
    "        W, b = params[-1]\n",
    "        outputs = np.dot(inputs, W) + b\n",
    "        return outputs\n",
    "\n",
    "    return init, apply\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the neural net\n",
    "def modified_MLP_II(layers, L_x=1.0, L_y=1.0, M_t=1, M_x=1, M_y=1, activation=relu):\n",
    "    def xavier_init(key, d_in, d_out):\n",
    "        glorot_stddev = 1. / np.sqrt((d_in + d_out) / 2.)\n",
    "        W = glorot_stddev * random.normal(key, (d_in, d_out))\n",
    "        b = np.zeros(d_out)\n",
    "        return W, b\n",
    "\n",
    "    w_x = 2.0 * np.pi / L_x\n",
    "    w_y = 2.0 * np.pi / L_y\n",
    "    k_x = np.arange(1, M_x + 1)\n",
    "    k_y = np.arange(1, M_y + 1)\n",
    "    k_xx, k_yy = np.meshgrid(k_x, k_y)\n",
    "    k_xx = k_xx.flatten()\n",
    "    k_yy = k_yy.flatten()\n",
    "\n",
    "    # Define input encoding function\n",
    "    def input_encoding(t, x, y):\n",
    "        k_t = np.power(10.0, np.arange(0, M_t + 1))\n",
    "        out = np.hstack([1, k_t * t,\n",
    "                         np.cos(k_x * w_x * x), np.cos(k_y * w_y * y),\n",
    "                         np.sin(k_x * w_x * x), np.sin(k_y * w_y * y),\n",
    "                         np.cos(k_xx * w_x * x) * np.cos(k_yy * w_y * y),\n",
    "                         np.cos(k_xx * w_x * x) * np.sin(k_yy * w_y * y),\n",
    "                         np.sin(k_xx * w_x * x) * np.cos(k_yy * w_y * y),\n",
    "                         np.sin(k_xx * w_x * x) * np.sin(k_yy * w_y * y)])\n",
    "        return out\n",
    "\n",
    "    def init(rng_key):\n",
    "        U1, b1 = xavier_init(random.PRNGKey(12345), layers[0], layers[1])\n",
    "        U2, b2 = xavier_init(random.PRNGKey(54321), layers[0], layers[1])\n",
    "\n",
    "        def init_layer(key, d_in, d_out):\n",
    "            k1, k2 = random.split(key)\n",
    "            W, b = xavier_init(k1, d_in, d_out)\n",
    "            return W, b\n",
    "\n",
    "        key, *keys = random.split(rng_key, len(layers))\n",
    "        params = list(map(init_layer, keys, layers[:-1], layers[1:]))\n",
    "        return (params, U1, b1, U2, b2)\n",
    "\n",
    "    def apply(params, inputs):\n",
    "        params, U1, b1, U2, b2 = params\n",
    "\n",
    "        t = inputs[0]\n",
    "        x = inputs[1]\n",
    "        y = inputs[2]\n",
    "        inputs = input_encoding(t, x, y)\n",
    "        U = activation(np.dot(inputs, U1) + b1)\n",
    "        V = activation(np.dot(inputs, U2) + b2)\n",
    "        for W, b in params[:-1]:\n",
    "            outputs = activation(np.dot(inputs, W) + b)\n",
    "            inputs = np.multiply(outputs, U) + np.multiply(1 - outputs, V)\n",
    "        W, b = params[-1]\n",
    "        outputs = np.dot(inputs, W) + b\n",
    "        return outputs\n",
    "\n",
    "    return init, apply\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the neural net\n",
    "def modified_MLP_III(layers, L_x=1.0, L_y=1.0, M_t=1, M_x=1, M_y=1, activation=relu):\n",
    "    def xavier_init(key, d_in, d_out):\n",
    "        glorot_stddev = 1. / np.sqrt((d_in + d_out) / 2.)\n",
    "        W = glorot_stddev * random.normal(key, (d_in, d_out))\n",
    "        b = np.zeros(d_out)\n",
    "        return W, b\n",
    "\n",
    "    w_x = 2.0 * np.pi / L_x\n",
    "    w_y = 2.0 * np.pi / L_y\n",
    "    k_x = np.arange(1, M_x + 1)\n",
    "    k_y = np.arange(1, M_y + 1)\n",
    "    k_xx, k_yy = np.meshgrid(k_x, k_y)\n",
    "    k_xx = k_xx.flatten()\n",
    "    k_yy = k_yy.flatten()\n",
    "\n",
    "    # Define input encoding function\n",
    "    def spatial_encoding(x, y, M_x, M_y):\n",
    "        out = np.hstack([1, np.cos(k_x * w_x * x), np.cos(k_y * w_y * y),\n",
    "                         np.sin(k_x * w_x * x), np.sin(k_y * w_y * y),\n",
    "                         np.cos(k_xx * w_x * x) * np.cos(k_yy * w_y * y),\n",
    "                         np.cos(k_xx * w_x * x) * np.sin(k_yy * w_y * y),\n",
    "                         np.sin(k_xx * w_x * x) * np.cos(k_yy * w_y * y),\n",
    "                         np.sin(k_xx * w_x * x) * np.sin(k_yy * w_y * y)])\n",
    "        return out\n",
    "\n",
    "    def temporal_encoding(t, M_t):\n",
    "        k = np.power(10.0, np.arange(0, M_t + 1))\n",
    "        out = k * t\n",
    "        return out\n",
    "\n",
    "    def init(rng_key):\n",
    "        U1, b1 = xavier_init(random.PRNGKey(12345), M_t + 1, layers[1])\n",
    "        U2, b2 = xavier_init(random.PRNGKey(54321), 2 * M_x + 2 * M_y + 4 * M_x * M_y + 1, layers[1])\n",
    "\n",
    "        def init_layer(key, d_in, d_out):\n",
    "            k1, k2 = random.split(key)\n",
    "            W, b = xavier_init(k1, d_in, d_out)\n",
    "            return W, b\n",
    "\n",
    "        key, *keys = random.split(rng_key, len(layers))\n",
    "        params = list(map(init_layer, keys, layers[:-1], layers[1:]))\n",
    "        return (params, U1, b1, U2, b2)\n",
    "\n",
    "    def apply(params, inputs):\n",
    "        params, U1, b1, U2, b2 = params\n",
    "        t = inputs[0]\n",
    "        x = inputs[1]\n",
    "        y = inputs[2]\n",
    "        H_t = temporal_encoding(t, M_t)\n",
    "        H_x = spatial_encoding(x, y, M_x, M_y)\n",
    "        inputs = np.hstack([H_t, H_x])\n",
    "        U = activation(np.dot(H_t, U1) + b1)\n",
    "        V = activation(np.dot(H_x, U2) + b2)\n",
    "        for W, b in params[:-1]:\n",
    "            outputs = activation(np.dot(inputs, W) + b)\n",
    "            inputs = np.multiply(outputs, U) + np.multiply(1 - outputs, V)\n",
    "        W, b = params[-1]\n",
    "        outputs = np.dot(inputs, W) + b\n",
    "        return outputs\n",
    "\n",
    "    return init, apply\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DataGenerator(data.Dataset):\n",
    "    def __init__(self, t0, t1, n_t=10, n_x=64, rng_key=random.PRNGKey(1234)):\n",
    "        'Initialization'\n",
    "        self.t0 = t0\n",
    "        self.t1 = t1 + 0.01 * t1\n",
    "        self.n_t = n_t\n",
    "        self.n_x = n_x\n",
    "        self.key = rng_key\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        self.key, subkey = random.split(self.key)\n",
    "        batch = self.__data_generation(subkey)\n",
    "        return batch\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def __data_generation(self, key):\n",
    "        'Generates data containing batch_size samples'\n",
    "        subkeys = random.split(key, 2)\n",
    "        t_r = random.uniform(subkeys[0], shape=(self.n_t,), minval=self.t0, maxval=self.t1).sort()\n",
    "        x_r = random.uniform(subkeys[1], shape=(self.n_x, 2), minval=0.0, maxval=2.0 * np.pi)\n",
    "        batch = (t_r, x_r)\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the model\n",
    "class PINN:\n",
    "    def __init__(self, key, w_exact, layers, M_t, M_x, M_y, state0, t0, t1, n_t, x_star, y_star, tol):\n",
    "\n",
    "        self.w_exact = w_exact\n",
    "\n",
    "        self.M_t = M_t\n",
    "        self.M_x = M_x\n",
    "        self.M_y = M_y\n",
    "\n",
    "        # grid\n",
    "        self.n_t = n_t\n",
    "        self.t0 = t0\n",
    "        self.t1 = t1\n",
    "        eps = 0.01 * t1\n",
    "        self.t = np.linspace(self.t0, self.t1 + eps, n_t)\n",
    "        self.x_star = x_star\n",
    "        self.y_star = y_star\n",
    "\n",
    "        # initial state\n",
    "        self.state0 = state0\n",
    "\n",
    "        self.tol = tol\n",
    "        self.M = np.triu(np.ones((n_t, n_t)), k=1).T\n",
    "\n",
    "        self.init, self.apply = modified_MLP_II(layers, L_x=2 * np.pi, L_y=2 * np.pi, M_t=M_t, M_x=M_x, M_y=M_y, activation=np.tanh)\n",
    "        params = self.init(rng_key=key)\n",
    "\n",
    "        # Use optimizers to set optimizer initialization and update functions\n",
    "        self.opt_init, self.opt_update, self.get_params = optimizers.adam(optimizers.exponential_decay(1e-3,\n",
    "                                                                                                       decay_steps=10000,\n",
    "                                                                                                       decay_rate=0.9))\n",
    "        self.opt_state = self.opt_init(params)\n",
    "        _, self.unravel = ravel_pytree(params)\n",
    "\n",
    "        self.u0_pred_fn = vmap(vmap(self.u_net, (None, None, None, 0)), (None, None, 0, None))\n",
    "        self.v0_pred_fn = vmap(vmap(self.v_net, (None, None, None, 0)), (None, None, 0, None))\n",
    "        self.w0_pred_fn = vmap(vmap(self.vorticity_net, (None, None, None, 0)), (None, None, 0, None))\n",
    "\n",
    "        self.u_pred_fn = vmap(vmap(vmap(self.u_net, (None, None, None, 0)), (None, None, 0, None)), (None, 0, None, None))\n",
    "        self.v_pred_fn = vmap(vmap(vmap(self.v_net, (None, None, None, 0)), (None, None, 0, None)), (None, 0, None, None))\n",
    "        self.w_pred_fn = vmap(vmap(vmap(self.vorticity_net, (None, None, None, 0)), (None, None, 0, None)),  (None, 0, None, None))\n",
    "        \n",
    "        # self.r_pred_fn = vmap(vmap(vmap(self.residual_net, (None, None, None, 0)), (None, None, 0, None)), (None, 0, None, None))\n",
    "        self.r_pred_fn = vmap(vmap(self.residual_net, (None, None, 0, 0)), (None, 0, None, None))\n",
    "\n",
    "        # Logger\n",
    "        self.itercount = itertools.count()\n",
    "\n",
    "        self.loss_log = []\n",
    "        self.loss_ics_log = []\n",
    "        self.loss_u0_log = []\n",
    "        self.loss_v0_log = []\n",
    "        self.loss_w0_log = []\n",
    "        self.loss_bcs_log = []\n",
    "        self.loss_res_w_log = []\n",
    "        self.loss_res_c_log = []\n",
    "        self.l2_error_log = []\n",
    "\n",
    "    def neural_net(self, params, t, x, y):\n",
    "        z = np.stack([t, x, y])\n",
    "        outputs = self.apply(params, z)\n",
    "        u = outputs[0]\n",
    "        v = outputs[1]\n",
    "        return u, v\n",
    "\n",
    "    def u_net(self, params, t, x, y):\n",
    "        u, _ = self.neural_net(params, t, x, y)\n",
    "        return u\n",
    "\n",
    "    def v_net(self, params, t, x, y):\n",
    "        _, v = self.neural_net(params, t, x, y)\n",
    "        return v\n",
    "\n",
    "    def vorticity_net(self, params, t, x, y):\n",
    "        u_y = grad(self.u_net, argnums=3)(params, t, x, y)\n",
    "        v_x = grad(self.v_net, argnums=2)(params, t, x, y)\n",
    "        w = v_x - u_y\n",
    "        return w\n",
    "\n",
    "    def residual_net(self, params, t, x, y):\n",
    "\n",
    "        u, v = self.neural_net(params, t, x, y)\n",
    "\n",
    "        u_x = grad(self.u_net, argnums=2)(params, t, x, y)\n",
    "        v_y = grad(self.v_net, argnums=3)(params, t, x, y)\n",
    "\n",
    "        w_t = grad(self.vorticity_net, argnums=1)(params, t, x, y)\n",
    "        w_x = grad(self.vorticity_net, argnums=2)(params, t, x, y)\n",
    "        w_y = grad(self.vorticity_net, argnums=3)(params, t, x, y)\n",
    "\n",
    "        w_xx = grad(grad(self.vorticity_net, argnums=2), argnums=2)(params, t, x, y)\n",
    "        w_yy = grad(grad(self.vorticity_net, argnums=3), argnums=3)(params, t, x, y)\n",
    "\n",
    "        res_w = w_t + u * w_x + v * w_y - nu * (w_xx + w_yy)\n",
    "        res_c = u_x + v_y\n",
    "\n",
    "        return res_w, res_c\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def residuals_and_weights(self, params, tol, batch):\n",
    "        t_r, x_r = batch\n",
    "        loss_u0, loss_v0, loss_w0 = self.loss_ics(params)\n",
    "        L_0 = 1e5 * (loss_u0 + loss_v0 + loss_w0)\n",
    "        res_w_pred, res_c_pred = self.r_pred_fn(params, t_r, x_r[:, 0], x_r[:, 1])\n",
    "        L_t = np.mean(res_w_pred ** 2 + 100 * res_c_pred ** 2, axis=1)\n",
    "        W = lax.stop_gradient(np.exp(- tol * (self.M @ L_t + L_0)))\n",
    "        return L_0, L_t, W\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def loss_ics(self, params):\n",
    "        # Compute forward pass\n",
    "        u0_pred = self.u0_pred_fn(params, 0.0, self.x_star, self.y_star)\n",
    "        v0_pred = self.v0_pred_fn(params, 0.0, self.x_star, self.y_star)\n",
    "        w0_pred = self.w0_pred_fn(params, 0.0, self.x_star, self.y_star)\n",
    "        # Compute loss\n",
    "        loss_u0 = np.mean((u0_pred - self.state0[0, :, :]) ** 2)\n",
    "        loss_v0 = np.mean((v0_pred - self.state0[1, :, :]) ** 2)\n",
    "        loss_w0 = np.mean((w0_pred - self.state0[2, :, :]) ** 2)\n",
    "        return loss_u0, loss_v0, loss_w0\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def loss_res(self, params, batch):\n",
    "        t_r, x_r = batch\n",
    "        # Compute forward pass\n",
    "        res_w_pred, res_c_pred = self.r_pred_fn(params, t_r, x_r[:, 0], x_r[:, 1])\n",
    "        # Compute loss\n",
    "        loss_res_w = np.mean(res_w_pred ** 2)\n",
    "        loss_res_c = np.mean(res_c_pred ** 2)\n",
    "        return loss_res_w, loss_res_c\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def loss(self, params, batch):\n",
    "\n",
    "        L_0, L_t, W = self.residuals_and_weights(params, self.tol, batch)\n",
    "        # Compute loss\n",
    "        loss = np.mean(W * L_t + L_0)\n",
    "        return loss\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def compute_l2_error(self, params):\n",
    "        w_pred = self.w_pred_fn(params, t_star[:num_step], x_star, y_star)\n",
    "        l2_error = np.linalg.norm(w_pred - self.w_exact) / np.linalg.norm(self.w_exact)\n",
    "        return l2_error\n",
    "\n",
    "    # Define a compiled update step\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def step(self, i, opt_state, batch):\n",
    "        params = self.get_params(opt_state)\n",
    "        g = grad(self.loss)(params, batch)\n",
    "        return self.opt_update(i, g, opt_state)\n",
    "\n",
    "    # Optimize parameters in a loop\n",
    "    def train(self, dataset, nIter=10000):\n",
    "        res_data = iter(dataset)\n",
    "        pbar = trange(nIter)\n",
    "        # Main training loop\n",
    "        for it in pbar:\n",
    "            batch = next(res_data)\n",
    "            self.current_count = next(self.itercount)\n",
    "            self.opt_state = self.step(self.current_count, self.opt_state, batch)\n",
    "\n",
    "            if it % 1000 == 0:\n",
    "                params = self.get_params(self.opt_state)\n",
    "\n",
    "                l2_error_value = self.compute_l2_error(params)\n",
    "\n",
    "                loss_value = self.loss(params, batch)\n",
    "\n",
    "                loss_u0_value, loss_v0_value, loss_w0_value = self.loss_ics(params)\n",
    "                loss_res_w_value, loss_res_c_value = self.loss_res(params, batch)\n",
    "                _, _, W_value = self.residuals_and_weights(params, tol, batch)\n",
    "\n",
    "                self.l2_error_log.append(l2_error_value)\n",
    "                self.loss_log.append(loss_value)\n",
    "                self.loss_u0_log.append(loss_u0_value)\n",
    "                self.loss_v0_log.append(loss_v0_value)\n",
    "                self.loss_w0_log.append(loss_w0_value)\n",
    "                self.loss_res_w_log.append(loss_res_w_value)\n",
    "                self.loss_res_c_log.append(loss_res_c_value)\n",
    "\n",
    "                pbar.set_postfix({'l2 error': l2_error_value,\n",
    "                                  'Loss': loss_value,\n",
    "                                  'loss_u0': loss_u0_value,\n",
    "                                  'loss_v0': loss_v0_value,\n",
    "                                  'loss_w0': loss_w0_value,\n",
    "                                  'loss_res_w': loss_res_w_value,\n",
    "                                  'loss_res_c': loss_res_c_value,\n",
    "                                  'W_min': W_value.min()})\n",
    "\n",
    "                if W_value.min() > 0.99:\n",
    "                    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = np.load('../data/NS.npy', allow_pickle=True).item()\n",
    "# Test data\n",
    "sol = data['sol']\n",
    "\n",
    "t_star = data['t']\n",
    "x_star = data['x']\n",
    "y_star = data['y']\n",
    "nu = data['viscosity']\n",
    "\n",
    "# downsampling\n",
    "sol = sol\n",
    "x_star = x_star\n",
    "y_star = y_star\n",
    "\n",
    "# Create PINNs model\n",
    "key = random.PRNGKey(1234)\n",
    "\n",
    "u0 = data['u0']\n",
    "v0 = data['v0']\n",
    "w0 = data['w0']\n",
    "state0 = np.stack([u0, v0, w0])\n",
    "M_t = 2\n",
    "M_x = 5\n",
    "M_y = 5\n",
    "d0 = 2 * M_x + 2 * M_y + 4 * M_x * M_y + M_t + 2\n",
    "layers = [d0, 128, 128, 128, 128, 2]\n",
    "\n",
    "num_step = 10\n",
    "t0 = 0.0\n",
    "t1 = t_star[num_step]\n",
    "n_t = 32\n",
    "tol = 1.0\n",
    "tol_list = [1e-3, 1e-2, 1e-1, 1e0]\n",
    "\n",
    "# Create data set\n",
    "n_x = 256\n",
    "dataset = DataGenerator(t0, t1, n_t, n_x)\n",
    "\n",
    "N = 3\n",
    "w_pred_list = []\n",
    "params_list = []\n",
    "losses_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Time: 1\n",
      "tol: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [03:27<00:00, 20.74s/it, l2 error=1.0516597, Loss=7357591.0, loss_u0=2.0256462, loss_v0=1.9009423, loss_w0=69.64932, loss_res_w=3281.2002, loss_res_c=12.049849, W_min=0.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tol: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:28<00:00, 14.86s/it, l2 error=0.6331852, Loss=1154932.0, loss_u0=0.52181125, loss_v0=0.60504425, loss_w0=10.422464, loss_res_w=4795.886, loss_res_c=14.388492, W_min=0.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tol: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:26<00:00, 14.62s/it, l2 error=0.46923387, Loss=232533.89, loss_u0=0.2738356, loss_v0=0.26360103, loss_w0=1.7879022, loss_res_w=8930.795, loss_res_c=12.367401, W_min=0.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tol: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:25<00:00, 14.54s/it, l2 error=0.45260793, Loss=150397.52, loss_u0=0.18903236, loss_v0=0.19860843, loss_w0=1.1163344, loss_res_w=8234.361, loss_res_c=12.132161, W_min=0.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative l2 error: 4.616e-01\n",
      "Final Time: 2\n",
      "tol: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [03:26<00:00, 20.60s/it, l2 error=1.0718802, Loss=2960976.2, loss_u0=3.0365896, loss_v0=1.4255565, loss_w0=25.147615, loss_res_w=3379.8152, loss_res_c=10.965466, W_min=0.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tol: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:26<00:00, 14.64s/it, l2 error=0.9159873, Loss=381775.94, loss_u0=1.481363, loss_v0=0.6761378, loss_w0=1.6602587, loss_res_w=4071.8022, loss_res_c=13.191718, W_min=0.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tol: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:26<00:00, 14.64s/it, l2 error=0.89943737, Loss=213466.95, loss_u0=0.6178883, loss_v0=0.3499188, loss_w0=1.1668625, loss_res_w=6047.2676, loss_res_c=12.010088, W_min=0.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tol: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:26<00:00, 14.65s/it, l2 error=0.9014215, Loss=67334.69, loss_u0=0.18522334, loss_v0=0.14770442, loss_w0=0.34041908, loss_res_w=4035.2295, loss_res_c=11.134968, W_min=0.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative l2 error: 7.094e-01\n",
      "Final Time: 3\n",
      "tol: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [03:26<00:00, 20.64s/it, l2 error=1.0867879, Loss=2857844.2, loss_u0=4.413732, loss_v0=2.2936993, loss_w0=21.871012, loss_res_w=3352.1636, loss_res_c=14.436405, W_min=0.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tol: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:26<00:00, 14.69s/it, l2 error=1.0780268, Loss=526176.7, loss_u0=2.4058905, loss_v0=1.3210081, loss_w0=1.5348681, loss_res_w=3601.3755, loss_res_c=14.030403, W_min=0.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tol: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:26<00:00, 14.65s/it, l2 error=1.0803432, Loss=270856.2, loss_u0=1.0432262, loss_v0=0.68471956, loss_w0=0.9806162, loss_res_w=4032.8066, loss_res_c=14.114986, W_min=0.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tol: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:26<00:00, 14.63s/it, l2 error=1.0605985, Loss=91849.71, loss_u0=0.32533702, loss_v0=0.28745958, loss_w0=0.30570048, loss_res_w=4862.479, loss_res_c=14.554125, W_min=0.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative l2 error: 8.350e-01\n",
      "Final Time: 4\n",
      "tol: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [03:24<00:00, 20.43s/it, l2 error=1.0953995, Loss=3180077.0, loss_u0=4.844303, loss_v0=2.7448761, loss_w0=24.211592, loss_res_w=3262.8633, loss_res_c=12.966481, W_min=0.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tol: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:25<00:00, 14.59s/it, l2 error=1.1495439, Loss=597617.94, loss_u0=2.6882389, loss_v0=1.6374471, loss_w0=1.6504934, loss_res_w=3495.2197, loss_res_c=13.3955, W_min=0.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tol: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:25<00:00, 14.57s/it, l2 error=1.1665772, Loss=320672.72, loss_u0=1.2070628, loss_v0=0.8787809, loss_w0=1.1208835, loss_res_w=4529.9004, loss_res_c=14.443423, W_min=0.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tol: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:25<00:00, 14.58s/it, l2 error=1.1338228, Loss=112960.516, loss_u0=0.40786695, loss_v0=0.38739675, loss_w0=0.33434147, loss_res_w=4821.0063, loss_res_c=14.032913, W_min=0.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative l2 error: 9.103e-01\n",
      "Final Time: 5\n",
      "tol: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [03:24<00:00, 20.48s/it, l2 error=1.1094579, Loss=3419291.8, loss_u0=5.3235855, loss_v0=3.1409216, loss_w0=25.728409, loss_res_w=2930.858, loss_res_c=14.07827, W_min=0.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tol: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:25<00:00, 14.54s/it, l2 error=1.1961359, Loss=672362.7, loss_u0=3.0262103, loss_v0=1.9238919, loss_w0=1.7735248, loss_res_w=3297.2065, loss_res_c=14.087459, W_min=0.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tol: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:45<03:02, 22.84s/it, l2 error=1.2183582, Loss=367648.12, loss_u0=1.4120839, loss_v0=1.0597967, loss_w0=1.2046007, loss_res_w=5071.5503, loss_res_c=14.456548, W_min=0.0]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/media/afrah2/MyWork/files2023/SK/CausalPINNs/NS/NS.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/afrah2/MyWork/files2023/SK/CausalPINNs/NS/NS.ipynb#W4sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mtol:\u001b[39m\u001b[39m'\u001b[39m, model\u001b[39m.\u001b[39mtol)\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/afrah2/MyWork/files2023/SK/CausalPINNs/NS/NS.ipynb#W4sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m# Train\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/media/afrah2/MyWork/files2023/SK/CausalPINNs/NS/NS.ipynb#W4sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     model\u001b[39m.\u001b[39mtrain(dataset, nIter\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/afrah2/MyWork/files2023/SK/CausalPINNs/NS/NS.ipynb#W4sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Store\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/afrah2/MyWork/files2023/SK/CausalPINNs/NS/NS.ipynb#W4sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m params \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mget_params(model\u001b[39m.\u001b[39mopt_state)\n",
      "\u001b[1;32m/media/afrah2/MyWork/files2023/SK/CausalPINNs/NS/NS.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/afrah2/MyWork/files2023/SK/CausalPINNs/NS/NS.ipynb#W4sZmlsZQ%3D%3D?line=160'>161</a>\u001b[0m batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(res_data)\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/afrah2/MyWork/files2023/SK/CausalPINNs/NS/NS.ipynb#W4sZmlsZQ%3D%3D?line=161'>162</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_count \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitercount)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/media/afrah2/MyWork/files2023/SK/CausalPINNs/NS/NS.ipynb#W4sZmlsZQ%3D%3D?line=162'>163</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopt_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_count, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopt_state, batch)\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/afrah2/MyWork/files2023/SK/CausalPINNs/NS/NS.ipynb#W4sZmlsZQ%3D%3D?line=164'>165</a>\u001b[0m \u001b[39mif\u001b[39;00m it \u001b[39m%\u001b[39m \u001b[39m1000\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/afrah2/MyWork/files2023/SK/CausalPINNs/NS/NS.ipynb#W4sZmlsZQ%3D%3D?line=165'>166</a>\u001b[0m     params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_params(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopt_state)\n",
      "File \u001b[0;32m~/anaconda3/envs/torchdiffeq/lib/python3.11/site-packages/jax/example_libraries/optimizers.py:120\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(data, xs)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[39m# The implementation here basically works by flattening pytrees. There are two\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[39m# levels of pytrees to think about: the pytree of params, which we can think of\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[39m# as defining an \"outer pytree\", and a pytree produced by applying init_fun to\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[39m# each leaf of the params pytree, which we can think of as the \"inner pytrees\".\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[39m# Since pytrees can be flattened, that structure is isomorphic to a list of\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[39m# lists (with no further nesting).\u001b[39;00m\n\u001b[1;32m    115\u001b[0m OptimizerState \u001b[39m=\u001b[39m namedtuple(\u001b[39m\"\u001b[39m\u001b[39mOptimizerState\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    116\u001b[0m                             [\u001b[39m\"\u001b[39m\u001b[39mpacked_state\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtree_def\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msubtree_defs\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    117\u001b[0m register_pytree_node(\n\u001b[1;32m    118\u001b[0m     OptimizerState,\n\u001b[1;32m    119\u001b[0m     \u001b[39mlambda\u001b[39;00m xs: ((xs\u001b[39m.\u001b[39mpacked_state,), (xs\u001b[39m.\u001b[39mtree_def, xs\u001b[39m.\u001b[39msubtree_defs)),\n\u001b[0;32m--> 120\u001b[0m     \u001b[39mlambda\u001b[39;00m data, xs: OptimizerState(xs[\u001b[39m0\u001b[39m], data[\u001b[39m0\u001b[39m], data[\u001b[39m1\u001b[39m]))  \u001b[39m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m    123\u001b[0m Array \u001b[39m=\u001b[39m Any\n\u001b[1;32m    124\u001b[0m Params \u001b[39m=\u001b[39m Any  \u001b[39m# Parameters are arbitrary nests of `jnp.ndarrays`.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for k in range(N):\n",
    "    # Initialize model\n",
    "    print('Final Time: {}'.format(k + 1))\n",
    "    w_exact = sol[num_step * k: num_step * (k + 1), :, :]\n",
    "    model = PINN(key, w_exact, layers, M_t, M_x, M_y, state0, t0, t1, n_t, x_star, y_star, tol)\n",
    "\n",
    "    # Train\n",
    "    for tol in tol_list:\n",
    "        model.tol = tol\n",
    "        print('tol:', model.tol)\n",
    "        # Train\n",
    "        model.train(dataset, nIter=10)\n",
    "\n",
    "    # Store\n",
    "    params = model.get_params(model.opt_state)\n",
    "    w_pred = model.w_pred_fn(params, t_star[:num_step], x_star, y_star)\n",
    "    w_pred_list.append(w_pred)\n",
    "    flat_params, _ = ravel_pytree(params)\n",
    "    params_list.append(flat_params)\n",
    "    losses_list.append([model.l2_error_log,\n",
    "                        model.loss_log,\n",
    "                        model.loss_u0_log,\n",
    "                        model.loss_v0_log,\n",
    "                        model.loss_w0_log,\n",
    "                        model.loss_res_w_log,\n",
    "                        model.loss_res_c_log, ])\n",
    "\n",
    "    np.save('causal_w_pred_list.npy', w_pred_list)\n",
    "    np.save('causal_params_list.npy', params_list)\n",
    "    np.save('causal_losses_list.npy', losses_list)\n",
    "\n",
    "    # error\n",
    "    w_preds = np.vstack(w_pred_list)\n",
    "    error = np.linalg.norm(w_preds - sol[:num_step * (k + 1), :, :]) / np.linalg.norm(sol[:num_step * (k + 1), :, :])\n",
    "    print('Relative l2 error: {:.3e}'.format(error))\n",
    "\n",
    "    params = model.get_params(model.opt_state)\n",
    "    u0_pred = model.u0_pred_fn(params, t1, x_star, y_star)\n",
    "    v0_pred = model.v0_pred_fn(params, t1, x_star, y_star)\n",
    "    w0_pred = model.w0_pred_fn(params, t1, x_star, y_star)\n",
    "    state0 = np.stack([u0_pred, v0_pred, w0_pred])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchdiffeq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
